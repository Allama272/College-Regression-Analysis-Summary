<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Obsidian Vault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>Obsidian Vault</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 26 Mar 2024 20:12:40 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 26 Mar 2024 20:12:39 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Least-Square-Estimation-Linear-Regression]]></title><description><![CDATA[ 
 <br>Tags: <a data-href="Regression" href="\Regression" class="internal-link" target="_self" rel="noopener">Regression</a> <a data-href="Computer Science" href="\Computer Science" class="internal-link" target="_self" rel="noopener">Computer Science</a> <a data-href="Math" href="\Math" class="internal-link" target="_self" rel="noopener">Math</a><br><br><br><img alt="Pasted image 20240314132935.png" src="\lib\media\pasted-image-20240314132935.png" style="width: 500px; max-width: 100%;">
 We want to minimize the error/residual (), between each point and the line
<img alt="Pasted image 20240314133050.png" src="\lib\media\pasted-image-20240314133050.png" style="width: 500px; max-width: 100%;">
At first thought, we can try summing these residuals. But some of those residuals are positive (above the line), and some are negative (below the line) making the sum of the errors a smaller number than it actually is.
Thus we square each residual, then sum them all.
<img alt="Pasted image 20240314133405.png" src="\lib\media\pasted-image-20240314133405.png" style="width: 500px; max-width: 100%;">
In order to minimize the error, and estimate  and  to get the best fit line<br>Imp
The LS regression line always passes through the centroid (, ) of the data
<br><br>we first isolate the error, then square the sum of the differences :<br><br>
<br>We then take the partial derivative with respect to  and 
<br>Set the partial derivative = 0
<br>Solve for  and  
Tip



<br><br><br><br><br><br>Note
note:  since it will be   
<br><br>Important
if we solved for y we would get

<br><br>The residuals can be written in many different forms:<br><br><br>Proof: <br><br><br><img alt="Pasted image 20240317140329.png" src="\lib\media\pasted-image-20240317140329.png" style="width: 400px; max-width: 100%;">
<img alt="Pasted image 20240317140429.png" src="\lib\media\pasted-image-20240317140429.png" style="width: 650px; max-width: 100%;"><br><br><br><a data-href="College Regression Analysis" href="\zettelkasten\hub-notes\college-regression-analysis.html" class="internal-link" target="_self" rel="noopener">College Regression Analysis</a>]]></description><link>zettelkasten\literature-notes\least-square-estimation-linear-regression.html</link><guid isPermaLink="false">Zettelkasten/Literature Notes/Least-Square-Estimation-Linear-Regression.md</guid><pubDate>Sun, 24 Mar 2024 16:00:37 GMT</pubDate><enclosure url="lib\media\pasted-image-20240314132935.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20240314132935.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Linear-Regression-Parameter-Distribution]]></title><description><![CDATA[ 
 <br>Tags:<a data-href="Regression" href="\Regression" class="internal-link" target="_self" rel="noopener">Regression</a> <a data-href="Computer Science" href="\Computer Science" class="internal-link" target="_self" rel="noopener">Computer Science</a> <a data-href="Math" href="\Math" class="internal-link" target="_self" rel="noopener">Math</a> <a data-href="Statistics" href="\Statistics" class="internal-link" target="_self" rel="noopener">Statistics</a> <a data-href="Data-Science" href="\Data-Science" class="internal-link" target="_self" rel="noopener">Data-Science</a><br><br><br>The  term is assumed to be an iid random variable that: <br>
<br>Is normally distributed 
<br>Has constant variance  at every value of  
<br> (independent)
<br>The model has the form  
<br>Reminder

Refer to: <a data-tooltip-position="top" aria-label="Least-Square-Estimation-Linear-Regression > $S_{xx}$ and $S_{xy}$ can be written as" data-href="Least-Square-Estimation-Linear-Regression#$S_{xx}$ and $S_{xy}$ can be written as" href="\zettelkasten\literature-notes\least-square-estimation-linear-regression.html#$S_{xx}$_and_$S_{xy}$_can_be_written_as" class="internal-link" target="_self" rel="noopener">Sxx &amp; Sxy Different forms</a>
<br><br><br>Pretty intuitive, this is the equation of the regression line. Where do you expect  to be? on the line that we got. <br><br><br>we can see that y is independent but not identically distributed, that is because the equation for the mean has a  which is different in each y making each point have a different mean ;and thus, a different distribution.<br>Note:  is fixed not a random variable so it has no distribution, i.e.  it is fixed in the sense that is taken to be known values. Thus, we treat x as a constant.<br><br><br><br> proof:
proof:
<br>
<br>for  proof, refer to:&nbsp;<a data-tooltip-position="top" aria-label="app://obsidian.md/Least-Square-Estimation-Linear-Regression#$S_%7Bxx%7D$%20and%20$S_%7Bxy%7D$%20can%20be%20written%20as" rel="noopener" class="external-link" href="\Least-Square-Estimation-Linear-Regression#$S_{xx}$ and $S_{xy}$ can be written as" target="_blank">Sxx &amp; Sxy Different forms</a>
<br>proof:
<br><br><br>Thus,  is an unbiased estimator.<br><br><br><br><br>reminder: <br><br> is an unbiased estimator<br>Reminder
when subtracted:

<br><br>For the cov =0 proof, refer to <a rel="noopener" class="external-link" href="https://stats.stackexchange.com/a/64217" target="_blank">https://stats.stackexchange.com/a/64217</a><br><br><br><br><a data-href="College Regression Analysis" href="\zettelkasten\hub-notes\college-regression-analysis.html" class="internal-link" target="_self" rel="noopener">College Regression Analysis</a>]]></description><link>zettelkasten\literature-notes\linear-regression-parameter-distribution.html</link><guid isPermaLink="false">Zettelkasten/Literature Notes/Linear-Regression-Parameter-Distribution.md</guid><pubDate>Tue, 26 Mar 2024 14:42:55 GMT</pubDate></item><item><title><![CDATA[College Regression Analysis]]></title><description><![CDATA[ 
 <br>
<br><a data-href="Least-Square-Estimation-Linear-Regression" href="\zettelkasten\literature-notes\least-square-estimation-linear-regression.html" class="internal-link" target="_self" rel="noopener">Least-Square-Estimation-Linear-Regression</a>
<br><a data-href="Linear-Regression-Parameter-Distribution" href="\zettelkasten\literature-notes\linear-regression-parameter-distribution.html" class="internal-link" target="_self" rel="noopener">Linear-Regression-Parameter-Distribution</a>
]]></description><link>zettelkasten\hub-notes\college-regression-analysis.html</link><guid isPermaLink="false">Zettelkasten/Hub Notes/College Regression Analysis.md</guid><pubDate>Tue, 26 Mar 2024 15:22:32 GMT</pubDate></item></channel></rss>